{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3a442fb-6c9a-4e8f-90c0-cbf12271cf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_DIR: C:\\Users\\Larchik\\wiki_movies_project\n",
      "OUT_CSV: C:\\Users\\Larchik\\movies.csv\n",
      "START_URL: https://ru.wikipedia.org/wiki/Категория:Фильмы_по_алфавиту\n"
     ]
    }
   ],
   "source": [
    "#A2 Параметры запуска (меняйте здесь)\n",
    "\n",
    "from pathlib import Path\n",
    "import sys, subprocess, os\n",
    "\n",
    "START_URL = \"https://ru.wikipedia.org/wiki/Категория:Фильмы_по_алфавиту\"\n",
    "\n",
    "# Ограничение по количеству фильмов для отладки (потом увеличите)\n",
    "MAX_FILMS = 50\n",
    "\n",
    "# 0 = без IMDb, 1 = с IMDb (через Wikidata -> IMDb)\n",
    "IMDB_ENABLED = 0\n",
    "\n",
    "# Папка проекта\n",
    "PROJECT_DIR = Path(\"wiki_movies_project\").resolve()\n",
    "OUT_CSV = Path(\"movies.csv\").resolve()\n",
    "\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
    "print(\"OUT_CSV:\", OUT_CSV)\n",
    "print(\"START_URL:\", START_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a1f935-8aeb-4bad-881f-b63914f49eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проект уже существует: C:\\Users\\Larchik\\wiki_movies_project\n",
      "CWD: C:\\Users\\Larchik\\wiki_movies_project\n"
     ]
    }
   ],
   "source": [
    "#A3 Создание Scrapy-проекта (как в слайдах: startproject)\n",
    "\n",
    "if not PROJECT_DIR.exists():\n",
    "    subprocess.run([sys.executable, \"-m\", \"scrapy\", \"startproject\", PROJECT_DIR.name], check=True)\n",
    "else:\n",
    "    print(\"Проект уже существует:\", PROJECT_DIR)\n",
    "\n",
    "# Переходим в папку проекта\n",
    "os.chdir(PROJECT_DIR)\n",
    "print(\"CWD:\", Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32ff49ab-1a4c-4c77-b563-448e7d8db2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spider saved to: wiki_movies_project\\spiders\\wiki_movies.py\n"
     ]
    }
   ],
   "source": [
    "#A4 Создание spider-файла (spiders/wiki_movies.py)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "spider_path = Path(PROJECT_DIR.name) / \"spiders\" / \"wiki_movies.py\"\n",
    "spider_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "spider_code = r'''\n",
    "import re\n",
    "import json\n",
    "import scrapy\n",
    "from scrapy.exceptions import CloseSpider\n",
    "\n",
    "\n",
    "def _clean_text(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.replace(\"\\xa0\", \" \")\n",
    "    s = re.sub(r\"\\[\\d+\\]\", \"\", s)          # убрать сноски [1]\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def _uniq_preserve(seq):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in seq:\n",
    "        if x not in seen:\n",
    "            seen.add(x)\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n",
    "\n",
    "class WikiMoviesSpider(scrapy.Spider):\n",
    "    name = \"wiki_movies\"\n",
    "    allowed_domains = [\"ru.wikipedia.org\", \"www.wikidata.org\", \"imdb.com\", \"www.imdb.com\"]\n",
    "    start_urls = []\n",
    "\n",
    "    custom_settings = {\n",
    "        # аккуратно по скорости (из слайдов: блокировки/ресурсы)\n",
    "        \"ROBOTSTXT_OBEY\": True,\n",
    "        \"DOWNLOAD_DELAY\": 0.5,\n",
    "        \"AUTOTHROTTLE_ENABLED\": True,\n",
    "        \"AUTOTHROTTLE_START_DELAY\": 0.5,\n",
    "        \"AUTOTHROTTLE_MAX_DELAY\": 10.0,\n",
    "        \"CONCURRENT_REQUESTS\": 8,\n",
    "\n",
    "        # кеш (чтобы не долбить сайт при отладке)\n",
    "        \"HTTPCACHE_ENABLED\": True,\n",
    "        \"HTTPCACHE_EXPIRATION_SECS\": 24 * 3600,\n",
    "\n",
    "        # лог\n",
    "        \"LOG_LEVEL\": \"INFO\",\n",
    "        \"USER_AGENT\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, start_url=None, max_films=200, imdb=0, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.start_urls = [start_url] if start_url else [\"https://ru.wikipedia.org/wiki/Категория:Фильмы_по_алфавиту\"]\n",
    "        self.max_films = int(max_films)\n",
    "\n",
    "        imdb_s = str(imdb).strip().lower()\n",
    "        self.imdb_enabled = imdb_s in {\"1\", \"true\", \"yes\", \"y\"}\n",
    "\n",
    "        self.seen_categories = set()\n",
    "        self.seen_films = set()\n",
    "        self.film_count = 0\n",
    "\n",
    "    # ---------- CATEGORY PARSING ----------\n",
    "\n",
    "    def parse(self, response):\n",
    "        yield from self.parse_category(response)\n",
    "\n",
    "    def parse_category(self, response):\n",
    "        if response.url in self.seen_categories:\n",
    "            return\n",
    "        self.seen_categories.add(response.url)\n",
    "\n",
    "        # 1) Подкатегории (если есть)\n",
    "        for href in response.xpath('//div[@id=\"mw-subcategories\"]//a/@href').getall():\n",
    "            url = response.urljoin(href)\n",
    "            if url not in self.seen_categories:\n",
    "                yield scrapy.Request(url, callback=self.parse_category)\n",
    "\n",
    "        # 2) Ссылки на страницы в категории\n",
    "        for a in response.xpath('//div[@id=\"mw-pages\"]//li/a[starts-with(@href, \"/wiki/\")]'):\n",
    "            href = a.xpath(\"./@href\").get()\n",
    "            title = _clean_text(a.xpath(\"string(.)\").get())\n",
    "\n",
    "            if not href:\n",
    "                continue\n",
    "\n",
    "            # фильтры мусора\n",
    "            if \":\" in href:                       # Категория:, Служебная:, Файл: и т.п.\n",
    "                continue\n",
    "            if title.startswith(\"Список \"):       # часто это не фильм\n",
    "                continue\n",
    "\n",
    "            url = response.urljoin(href)\n",
    "            if url in self.seen_films:\n",
    "                continue\n",
    "\n",
    "            self.seen_films.add(url)\n",
    "\n",
    "            # лимит по количеству фильмов (для отладки)\n",
    "            if self.film_count >= self.max_films:\n",
    "                raise CloseSpider(f\"Reached max_films={self.max_films}\")\n",
    "\n",
    "            yield scrapy.Request(url, callback=self.parse_film)\n",
    "\n",
    "        # 3) Пагинация категории (\"Следующая страница\")\n",
    "        next_href = response.xpath('//a[contains(., \"Следующая страница\")]/@href').get()\n",
    "        if next_href:\n",
    "            yield scrapy.Request(response.urljoin(next_href), callback=self.parse_category)\n",
    "\n",
    "    # ---------- FILM PAGE PARSING ----------\n",
    "\n",
    "    def _infobox_td(self, response, labels):\n",
    "        \"\"\"\n",
    "        Находит td в инфобоксе по заголовку th, содержащему один из labels.\n",
    "        \"\"\"\n",
    "        for label in labels:\n",
    "            td = response.xpath(\n",
    "                f'//table[contains(@class,\"infobox\")]//tr[th//text()[contains(., \"{label}\")]]/td[1]'\n",
    "            )\n",
    "            if td and td.get():\n",
    "                return td\n",
    "        return None\n",
    "\n",
    "    def _td_to_value(self, td_sel):\n",
    "        \"\"\"\n",
    "        Превращает td в значение: предпочитаем тексты ссылок (они аккуратнее),\n",
    "        иначе берём весь текст.\n",
    "        \"\"\"\n",
    "        if td_sel is None:\n",
    "            return \"\"\n",
    "\n",
    "        link_texts = [t.strip() for t in td_sel.xpath('.//a//text()').getall() if t.strip()]\n",
    "        link_texts = [t for t in link_texts if t not in {\"[\", \"]\"}]\n",
    "        link_texts = _uniq_preserve(link_texts)\n",
    "        if link_texts:\n",
    "            return _clean_text(\"; \".join(link_texts))\n",
    "\n",
    "        raw = \" \".join([t.strip() for t in td_sel.xpath(\".//text()\").getall() if t.strip()])\n",
    "        return _clean_text(raw)\n",
    "\n",
    "    def _extract_year(self, text):\n",
    "        text = _clean_text(text)\n",
    "        m = re.search(r\"(18|19|20)\\d{2}\", text)\n",
    "        return m.group(0) if m else \"\"\n",
    "\n",
    "    def parse_film(self, response):\n",
    "        # базовая проверка: есть ли инфобокс\n",
    "        infobox = response.xpath('//table[contains(@class,\"infobox\")]')\n",
    "        if not infobox or not infobox.get():\n",
    "            return\n",
    "\n",
    "        title = _clean_text(response.xpath('string(//h1[@id=\"firstHeading\"])').get())\n",
    "        wiki_url = response.url\n",
    "\n",
    "        genre = self._td_to_value(self._infobox_td(response, [\"Жанр\", \"Жанры\"]))\n",
    "        director = self._td_to_value(self._infobox_td(response, [\"Режиссёр\", \"Режиссер\", \"Режиссёры\", \"Режиссеры\"]))\n",
    "        country = self._td_to_value(self._infobox_td(response, [\"Страна\", \"Страны\"]))\n",
    "\n",
    "        year_raw = self._td_to_value(self._infobox_td(response, [\"Год\", \"Годы\", \"Премьера\", \"Дата выхода\"]))\n",
    "        year = self._extract_year(year_raw)\n",
    "\n",
    "        item = {\n",
    "            \"title\": title,\n",
    "            \"genre\": genre,\n",
    "            \"director\": director,\n",
    "            \"country\": country,\n",
    "            \"year\": year,\n",
    "            \"wiki_url\": wiki_url,\n",
    "        }\n",
    "\n",
    "        # счётчик (факт успешной обработки фильма)\n",
    "        self.film_count += 1\n",
    "\n",
    "        # IMDb (опционально): Wikipedia -> Wikidata -> IMDb\n",
    "        if not self.imdb_enabled:\n",
    "            yield item\n",
    "            return\n",
    "\n",
    "        wikidata_href = response.xpath('//li[@id=\"t-wikibase\"]/a/@href').get()\n",
    "        if not wikidata_href or \"wikidata.org/wiki/\" not in wikidata_href:\n",
    "            # fallback: без imdb\n",
    "            item[\"imdb_id\"] = \"\"\n",
    "            item[\"imdb_rating\"] = \"\"\n",
    "            yield item\n",
    "            return\n",
    "\n",
    "        qid = wikidata_href.rsplit(\"/\", 1)[-1].split(\"#\")[0].strip()\n",
    "        wd_json_url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json?flavor=dump\"\n",
    "\n",
    "        yield scrapy.Request(\n",
    "            wd_json_url,\n",
    "            callback=self.parse_wikidata,\n",
    "            meta={\"item\": item, \"qid\": qid},\n",
    "        )\n",
    "\n",
    "    def parse_wikidata(self, response):\n",
    "        item = response.meta[\"item\"]\n",
    "        qid = response.meta[\"qid\"]\n",
    "\n",
    "        try:\n",
    "            data = json.loads(response.text)\n",
    "            ent = data[\"entities\"][qid]\n",
    "            claims = ent.get(\"claims\", {})\n",
    "            p345 = claims.get(\"P345\", [])\n",
    "            imdb_id = \"\"\n",
    "            if p345:\n",
    "                imdb_id = p345[0][\"mainsnak\"][\"datavalue\"][\"value\"]\n",
    "            imdb_id = str(imdb_id).strip()\n",
    "        except Exception:\n",
    "            imdb_id = \"\"\n",
    "\n",
    "        item[\"imdb_id\"] = imdb_id\n",
    "\n",
    "        if not imdb_id:\n",
    "            item[\"imdb_rating\"] = \"\"\n",
    "            yield item\n",
    "            return\n",
    "\n",
    "        imdb_url = f\"https://www.imdb.com/title/{imdb_id}/\"\n",
    "        item[\"imdb_url\"] = imdb_url\n",
    "\n",
    "        yield scrapy.Request(\n",
    "            imdb_url,\n",
    "            callback=self.parse_imdb,\n",
    "            meta={\"item\": item},\n",
    "            headers={\"Accept-Language\": \"en-US,en;q=0.9\"},\n",
    "        )\n",
    "\n",
    "    def parse_imdb(self, response):\n",
    "        item = response.meta[\"item\"]\n",
    "\n",
    "        rating = \"\"\n",
    "        # На IMDb часто есть JSON-LD со структурой и aggregateRating\n",
    "        scripts = response.xpath('//script[@type=\"application/ld+json\"]/text()').getall()\n",
    "        for s in scripts:\n",
    "            s = s.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            try:\n",
    "                js = json.loads(s)\n",
    "                if isinstance(js, list):\n",
    "                    # иногда несколько объектов\n",
    "                    for obj in js:\n",
    "                        ar = obj.get(\"aggregateRating\", {}) if isinstance(obj, dict) else {}\n",
    "                        if ar.get(\"ratingValue\"):\n",
    "                            rating = str(ar.get(\"ratingValue\"))\n",
    "                            break\n",
    "                elif isinstance(js, dict):\n",
    "                    ar = js.get(\"aggregateRating\", {})\n",
    "                    if ar.get(\"ratingValue\"):\n",
    "                        rating = str(ar.get(\"ratingValue\"))\n",
    "                if rating:\n",
    "                    break\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        item[\"imdb_rating\"] = rating\n",
    "        yield item\n",
    "'''\n",
    "\n",
    "spider_path.write_text(spider_code, encoding=\"utf-8\")\n",
    "print(\"Spider saved to:\", spider_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "636390b2-806a-4dff-ba66-af258458afab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK patched: wiki_movies_project\\spiders\\wiki_movies.py\n"
     ]
    }
   ],
   "source": [
    "#B5 Патч spider: фикс реактора для Windows + убираем CloseSpider по лимиту\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "spider_path = Path(\"wiki_movies_project\") / \"spiders\" / \"wiki_movies.py\"\n",
    "text = spider_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "# 1) Добавляем TWISTED_REACTOR в custom_settings (если ещё не добавлен)\n",
    "if \"TWISTED_REACTOR\" not in text:\n",
    "    text = text.replace(\n",
    "        '\"USER_AGENT\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\",',\n",
    "        '\"USER_AGENT\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\",\\n'\n",
    "        '        \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",'\n",
    "    )\n",
    "\n",
    "# 2) Убираем raise CloseSpider(...) чтобы не закрывать процесс \"ошибкой\"\n",
    "text = text.replace(\n",
    "    'raise CloseSpider(f\"Reached max_films={self.max_films}\")',\n",
    "    'return'\n",
    ")\n",
    "\n",
    "spider_path.write_text(text, encoding=\"utf-8\")\n",
    "print(\"OK patched:\", spider_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e181db3e-ebce-46de-aea8-7e17b31f14aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 19:38:28 [scrapy.utils.log] INFO: Scrapy 2.8.0 started (bot: wiki_movies_project)\n",
      "2026-01-30 19:38:28 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.4, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 24.0.0 (OpenSSL 3.0.13 30 Jan 2024), cryptography 42.0.2, Platform Windows-10-10.0.19045-SP0\n",
      "2026-01-30 19:38:28 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'AUTOTHROTTLE_MAX_DELAY': 10.0,\n",
      " 'AUTOTHROTTLE_START_DELAY': 0.5,\n",
      " 'BOT_NAME': 'wiki_movies_project',\n",
      " 'CONCURRENT_REQUESTS': 8,\n",
      " 'DOWNLOAD_DELAY': 0.5,\n",
      " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
      " 'HTTPCACHE_ENABLED': True,\n",
      " 'HTTPCACHE_EXPIRATION_SECS': 86400,\n",
      " 'LOG_LEVEL': 'INFO',\n",
      " 'NEWSPIDER_MODULE': 'wiki_movies_project.spiders',\n",
      " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['wiki_movies_project.spiders'],\n",
      " 'TELNETCONSOLE_ENABLED': False,\n",
      " 'TWISTED_REACTOR': 'twisted.internet.selectreactor.SelectReactor',\n",
      " 'USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, '\n",
      "               'like Gecko) Chrome/120 Safari/537.36'}\n",
      "2026-01-30 19:38:28 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.throttle.AutoThrottle']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: C:\\Users\\Larchik\\wiki_movies_project | scrapy.cfg: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 19:38:28 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats',\n",
      " 'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware']\n",
      "2026-01-30 19:38:28 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2026-01-30 19:38:28 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2026-01-30 19:38:28 [scrapy.core.engine] INFO: Spider opened\n",
      "2026-01-30 19:38:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2026-01-30 19:38:29 [tldextract.cache] WARNING: unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache/publicsuffix.org-tlds\\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Отказано в доступе: 'C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\site-packages\\\\tldextract\\\\.suffix_cache'\n",
      "2026-01-30 19:38:29 [py.warnings] WARNING: C:\\ProgramData\\anaconda3\\Lib\\site-packages\\scrapy\\selector\\unified.py:83: UserWarning: Selector got both text and root, root is being ignored.\n",
      "  super().__init__(text=text, type=st, root=root, **kwargs)\n",
      "\n",
      "2026-01-30 19:39:29 [scrapy.extensions.logstats] INFO: Crawled 124 pages (at 124 pages/min), scraped 120 items (at 120 items/min)\n",
      "2026-01-30 19:40:29 [scrapy.extensions.logstats] INFO: Crawled 217 pages (at 93 pages/min), scraped 213 items (at 93 items/min)\n",
      "2026-01-30 19:41:29 [scrapy.extensions.logstats] INFO: Crawled 254 pages (at 37 pages/min), scraped 250 items (at 37 items/min)\n",
      "2026-01-30 19:42:29 [scrapy.extensions.logstats] INFO: Crawled 302 pages (at 48 pages/min), scraped 298 items (at 48 items/min)\n",
      "2026-01-30 19:43:29 [scrapy.extensions.logstats] INFO: Crawled 346 pages (at 44 pages/min), scraped 342 items (at 44 items/min)\n",
      "2026-01-30 19:44:20 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2026-01-30 19:44:20 [scrapy.extensions.feedexport] INFO: Stored csv feed (515 items) in: movies.csv\n",
      "2026-01-30 19:44:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/exception_count': 3,\n",
      " 'downloader/exception_type_count/scrapy.exceptions.IgnoreRequest': 3,\n",
      " 'downloader/request_bytes': 430657,\n",
      " 'downloader/request_count': 520,\n",
      " 'downloader/request_method_count/GET': 520,\n",
      " 'downloader/response_bytes': 13317109,\n",
      " 'downloader/response_count': 520,\n",
      " 'downloader/response_status_count/200': 520,\n",
      " 'elapsed_time_seconds': 351.338344,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2026, 1, 30, 16, 44, 20, 447007),\n",
      " 'httpcache/firsthand': 247,\n",
      " 'httpcache/hit': 273,\n",
      " 'httpcache/miss': 247,\n",
      " 'httpcache/store': 247,\n",
      " 'httpcompression/response_bytes': 56379295,\n",
      " 'httpcompression/response_count': 520,\n",
      " 'item_scraped_count': 515,\n",
      " 'log_count/INFO': 14,\n",
      " 'log_count/WARNING': 2,\n",
      " 'request_depth_max': 2,\n",
      " 'response_received_count': 520,\n",
      " 'robotstxt/forbidden': 3,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/200': 1,\n",
      " 'scheduler/dequeued': 522,\n",
      " 'scheduler/dequeued/memory': 522,\n",
      " 'scheduler/enqueued': 522,\n",
      " 'scheduler/enqueued/memory': 522,\n",
      " 'start_time': datetime.datetime(2026, 1, 30, 16, 38, 29, 108663)}\n",
      "2026-01-30 19:44:20 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE. CSV: C:\\Users\\Larchik\\wiki_movies_project\\movies.csv exists: True size: 131704\n"
     ]
    }
   ],
   "source": [
    "#C4 Запуск Scrapy из Jupyter так, чтобы CSV точно создавался (Windows-safe)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "# 1) Убедимся, что мы в корне проекта (где scrapy.cfg)\n",
    "print(\"CWD:\", Path.cwd(), \"| scrapy.cfg:\", Path(\"scrapy.cfg\").exists())\n",
    "\n",
    "# 2) Пишем В ОТНОСИТЕЛЬНЫЙ файл (иначе Scrapy на Windows воспринимает C:\\ как URI-схему \"c\")\n",
    "OUT_CSV_LOCAL = Path(\"movies.csv\")        # относительный путь в корне проекта\n",
    "OUT_CSV_ABS = (Path.cwd() / OUT_CSV_LOCAL).resolve()\n",
    "\n",
    "# чистим старый файл\n",
    "if OUT_CSV_LOCAL.exists():\n",
    "    OUT_CSV_LOCAL.unlink()\n",
    "\n",
    "# 3) Настройки проекта + FEEDS\n",
    "settings = get_project_settings()\n",
    "settings.set(\"FEEDS\", {\n",
    "    str(OUT_CSV_LOCAL): {\"format\": \"csv\", \"encoding\": \"utf-8\", \"overwrite\": True}\n",
    "})\n",
    "settings.set(\"TELNETCONSOLE_ENABLED\", False)  # чтобы не занимал порт\n",
    "\n",
    "# 4) Запуск без сигнал-хендлеров (иначе падает _handleSignals)\n",
    "from wiki_movies_project.spiders.wiki_movies import WikiMoviesSpider\n",
    "\n",
    "process = CrawlerProcess(settings)\n",
    "process.crawl(\n",
    "    WikiMoviesSpider,\n",
    "    start_url=START_URL,\n",
    "    max_films=MAX_FILMS,\n",
    "    imdb=IMDB_ENABLED\n",
    ")\n",
    "process.start(install_signal_handlers=False)\n",
    "\n",
    "print(\"DONE. CSV:\", OUT_CSV_ABS, \"exists:\", OUT_CSV_ABS.exists(), \"size:\", OUT_CSV_ABS.stat().st_size if OUT_CSV_ABS.exists() else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed15bce3-cba8-4d88-ad89-aee27e66ff7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 19:44:20 [numexpr.utils] INFO: NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>director</th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>wiki_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1+1 (фильм)</td>\n",
       "      <td>комедийная драма; бадди-муви</td>\n",
       "      <td>Оливье Накаш; 2; Эрик Толедано</td>\n",
       "      <td>Франция</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>https://ru.wikipedia.org/wiki/1%2B1_(%D1%84%D0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-99</td>\n",
       "      <td>комедия</td>\n",
       "      <td>Амасий Мартиросян</td>\n",
       "      <td>СССР</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>https://ru.wikipedia.org/wiki/01-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 % (фильм)</td>\n",
       "      <td>криминальный</td>\n",
       "      <td>Стив МакКаллум</td>\n",
       "      <td>Австралия</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>https://ru.wikipedia.org/wiki/1_%25_(%D1%84%D0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 миля до тебя</td>\n",
       "      <td>драма; мелодрама</td>\n",
       "      <td>Лейф Тильден</td>\n",
       "      <td>США</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>https://ru.wikipedia.org/wiki/1_%D0%BC%D0%B8%D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0-41*</td>\n",
       "      <td>документальный; драма; комедия</td>\n",
       "      <td>Сенна Хегде</td>\n",
       "      <td>Индия</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>https://ru.wikipedia.org/wiki/0-41*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1 (документальный фильм)</td>\n",
       "      <td>документальный</td>\n",
       "      <td>Пол Краудер</td>\n",
       "      <td>США</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>https://ru.wikipedia.org/wiki/1_(%D0%B4%D0%BE%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(Не)бывшие</td>\n",
       "      <td>драма; мелодрама</td>\n",
       "      <td>Стефан Бризе</td>\n",
       "      <td>Франция</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>https://ru.wikipedia.org/wiki/(%D0%9D%D0%B5)%D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Не)идеальные роботы</td>\n",
       "      <td>романтическая комедия; научно-фантастический ф...</td>\n",
       "      <td>Энтони Хайнс; Каспер Кристенсен</td>\n",
       "      <td>США</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>https://ru.wikipedia.org/wiki/(%D0%9D%D0%B5)%D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>«SOS» над тайгой</td>\n",
       "      <td>драма; приключения</td>\n",
       "      <td>Аркадий Кольцатый</td>\n",
       "      <td>СССР</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>https://ru.wikipedia.org/wiki/%C2%ABSOS%C2%BB_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>«Москвич», любовь моя</td>\n",
       "      <td>драма</td>\n",
       "      <td>Арам Шахбазян</td>\n",
       "      <td>Армения; Франция; Россия</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>https://ru.wikipedia.org/wiki/%C2%AB%D0%9C%D0%...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      title  \\\n",
       "0               1+1 (фильм)   \n",
       "1                     01-99   \n",
       "2               1 % (фильм)   \n",
       "3            1 миля до тебя   \n",
       "4                     0-41*   \n",
       "5  1 (документальный фильм)   \n",
       "6                (Не)бывшие   \n",
       "7      (Не)идеальные роботы   \n",
       "8          «SOS» над тайгой   \n",
       "9     «Москвич», любовь моя   \n",
       "\n",
       "                                               genre  \\\n",
       "0                       комедийная драма; бадди-муви   \n",
       "1                                            комедия   \n",
       "2                                       криминальный   \n",
       "3                                   драма; мелодрама   \n",
       "4                     документальный; драма; комедия   \n",
       "5                                     документальный   \n",
       "6                                   драма; мелодрама   \n",
       "7  романтическая комедия; научно-фантастический ф...   \n",
       "8                                 драма; приключения   \n",
       "9                                              драма   \n",
       "\n",
       "                          director                   country    year  \\\n",
       "0   Оливье Накаш; 2; Эрик Толедано                   Франция  2011.0   \n",
       "1                Амасий Мартиросян                      СССР  1959.0   \n",
       "2                   Стив МакКаллум                 Австралия  2017.0   \n",
       "3                     Лейф Тильден                       США  2017.0   \n",
       "4                      Сенна Хегде                     Индия  2016.0   \n",
       "5                      Пол Краудер                       США  2013.0   \n",
       "6                     Стефан Бризе                   Франция  2023.0   \n",
       "7  Энтони Хайнс; Каспер Кристенсен                       США  2023.0   \n",
       "8                Аркадий Кольцатый                      СССР  1976.0   \n",
       "9                    Арам Шахбазян  Армения; Франция; Россия  2015.0   \n",
       "\n",
       "                                            wiki_url  \n",
       "0  https://ru.wikipedia.org/wiki/1%2B1_(%D1%84%D0...  \n",
       "1                https://ru.wikipedia.org/wiki/01-99  \n",
       "2  https://ru.wikipedia.org/wiki/1_%25_(%D1%84%D0...  \n",
       "3  https://ru.wikipedia.org/wiki/1_%D0%BC%D0%B8%D...  \n",
       "4                https://ru.wikipedia.org/wiki/0-41*  \n",
       "5  https://ru.wikipedia.org/wiki/1_(%D0%B4%D0%BE%...  \n",
       "6  https://ru.wikipedia.org/wiki/(%D0%9D%D0%B5)%D...  \n",
       "7  https://ru.wikipedia.org/wiki/(%D0%9D%D0%B5)%D...  \n",
       "8  https://ru.wikipedia.org/wiki/%C2%ABSOS%C2%BB_...  \n",
       "9  https://ru.wikipedia.org/wiki/%C2%AB%D0%9C%D0%...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows: 515\n",
      "\n",
      "Missing (%) by column:\n",
      "year        6.8\n",
      "country     5.2\n",
      "genre       4.5\n",
      "director    0.6\n",
      "title       0.0\n",
      "wiki_url    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#C5 Читаем CSV и смотрим первые строки\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(OUT_CSV_ABS)\n",
    "display(df.head(10))\n",
    "\n",
    "print(\"\\nRows:\", len(df))\n",
    "print(\"\\nMissing (%) by column:\")\n",
    "print((df.isna().mean() * 100).round(1).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d20de2-4c0b-40c7-8b72-fa1e36ffba47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901efc84-181d-4cf1-847f-62050a236c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dfa31c-13b4-4c02-8364-3834f473394c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce5362-e554-4161-a3cb-87edd71d469a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453deb03-f012-4213-ab73-754280117d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b744275-fc1b-4791-9142-349beb41157a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73163282-4a78-4e56-bc44-9c564daa4887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefbfde9-44da-45d6-9359-6b075ab37d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4815b14-7ad4-4bc3-a40c-674c5120f166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
